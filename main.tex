\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath,amsthm,enumitem}
\usepackage{bbm}
\usepackage{graphicx}
\usepackage{lscape}
\usepackage{geometry}
\usepackage{color}
\usepackage{float}
\usepackage{listings}
\usepackage{amssymb}
\usepackage{verbatim}
\usepackage{algpseudocode} 
\usepackage{algorithm}  
\usepackage{algorithmicx}  
\usepackage{hyperref}
\usepackage{float}
\renewcommand{\baselinestretch}{1.5}
\usepackage{todonotes}
\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm} 

\title{Note on Machine Learning}
\author{Haocheng Dai}
\date{}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
% \newtheorem{definition}{Example}

\algnewcommand{\Inputs}[1]{%
  \State \textbf{Inputs:}\hspace*{\algorithmicindent}\parbox[t]{.8\linewidth}{\raggedright #1}
}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\newtheorem{corollary}{Corollary}
\newenvironment{example}[1][]{\refstepcounter{example}\par\medskip
   \noindent \textbf{Example~\theexample. #1} \rmfamily}{\medskip}
\newcounter{example}[Example]
      
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator\supp{supp}
\begin{document}
\maketitle

\section{Statistics}
\begin{definition}
\boxed{\textbf{Likelihood}} is the probability of certain observation given the reason, namely
\begin{equation*}
    p(\text{observation}|\text{reason}).
\end{equation*}
\end{definition}

\begin{definition}
\boxed{\textbf{Prior probability}} is the probability of the reason without any observation, namely
\begin{equation*}
    p(\text{reason}).
\end{equation*}
\end{definition}

\begin{definition}
\boxed{\textbf{Posterior probability}} is the probability of the reason with certain observation, namely
\begin{align*}
    p(\text{reason}|\text{observation})&=\frac{p(\text{observation}|\text{reason})}{p(\text{observation})}\times p(\text{reason})\\
    &\propto \text{Likelihood}\times\text{Prior Probability}
\end{align*}
\end{definition}

\begin{definition}
\boxed{\textbf{Probability mass function(PMF)}} is a discrete function $f(x)$ that gives the probability that a discrete random variable is exactly equal to some value. The probability is acquired through
\begin{equation*}
    P(X=a)=f(a).
\end{equation*}
\end{definition}

\begin{definition}
\boxed{\textbf{Probability density function(PDF)}} is a continuous function $f(x)$ the gives the probability of a continuous random variable is located among certain interval. The probability is acquired through
\begin{equation*}
    P(a\leq X\leq b)=\int _{a}^{b}f(x)\,dx.
\end{equation*}
\end{definition}

\begin{example}
Suppose bacteria of a certain species typically live 4 to 6 hours. The probability that a bacterium lives exactly 5 hours is equal to zero. A lot of bacteria live for approximately 5 hours, but there is no chance that any given bacterium dies at exactly 5.0000000000... hours. However, the probability that the bacterium dies between 5 hours and 5.01 hours is quantifiable. 
\end{example}

\begin{remark}
A PDF must be integrated over an interval to yield a probability. 
\end{remark}

\begin{definition}
\boxed{\textbf{Cumulative density function(CDF)}} is a continuous function $F(x)$ the gives the probability of a continuous random variable is less than or equal to $x$.
\begin{align*}
    F(x)&=\int ^{x}_{-\infty}f(t)\,dt.\\
    P(X\leq b)&=F(b)
\end{align*}
\end{definition}

\begin{remark}
PDF is the derivative of the CDF.
\end{remark}

\begin{definition}
\boxed{\textbf{Maximum likelihood estimation(MLE)}} is a method of estimating the parameters of a probability distribution by maximizing a likelihood function, so that under the assumed statistical model the observed data is most probable.
\end{definition}

\begin{example}
Now suppose that there was only one coin but its $p$ could have been any value $0\le p\le 1$. The likelihood function to be maximized is
\begin{equation*}
    L(p)={\binom {80}{49}}p^{49}(1-p)^{31}
\end{equation*}
and the maximization is over all possible values $0\le p\le 1$.

By differentiating $L(p)$ with respect to $p$ and setting to zero, we yield that the maximum likelihood estimator for p is $\frac{49}{80}$.
\end{example}

\begin{definition}
\boxed{\textbf{Monte Carlo methods}} are a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results. Monte Carlo methods vary, but tend to follow a particular pattern:
\begin{enumerate}
    \item Define a domain of possible inputs;
    \item Generate inputs randomly from a probability distribution over the domain;
    \item Perform a deterministic computation on the inputs;
    \item Aggregate the results.
\end{enumerate}
\end{definition}

\begin{example}
Consider a quadrant (circular sector) inscribed in a unit square. Given that the ratio of their areas is $\frac{\pi}{4}$, the value of $\pi$ can be approximated using a Monte Carlo method:
\begin{enumerate}
    \item Draw a square, then inscribe a quadrant within it;
    \item Uniformly scatter a given number of points over the square;
    \item Count the number of points inside the quadrant, i.e. having a distance from the origin of less than 1. The ratio of the inside-count and the total-sample-count is an estimate of the ratio of the two areas, namely the approximation of $\frac{\pi}{4}$;
    \item Multiply the result by 4 to estimate $\pi$.
\end{enumerate}
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.7]{figure/estimating-pi-monte-carlo-method.png}
    % \caption{Caption}
    % \label{fig:my_label}
\end{figure}
\end{example}

\begin{definition}
\boxed{\textbf{Normal distribution}} $X\sim N(\mu,\sigma^2)$ is defined as 
\begin{equation*}
     f(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}, F(x)=\int_0^\infty\frac{1}{\sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}dx,
 \end{equation*}
 where $\mu$ is the mean value and $\sigma$ is the standard deviation.
\end{definition}

\begin{remark}
Some properties of normal distribution:
\begin{itemize}
    \item If $X\sim N(\mu,\sigma^2), a, b$ are real, then $aX+b\sim N(a\mu+b,(a\sigma)^2)$.
    \item If $X\sim N(\mu_X,\sigma_X^2)$ and $Y\sim N(\mu_Y,\sigma_Y^2)$ are two independent random variables, then $ X+Y \sim N(\mu_X + \mu_Y, \sigma^2_X + \sigma^2_Y), {\displaystyle X-Y\sim N(\mu _{X}-\mu _{Y},\sigma _{X}^{2}+\sigma _{Y}^{2})}$.
\end{itemize}
\end{remark}

\begin{theorem}
\textbf{Law of large numbers(LLN).} When $\{X_1,\cdots,X_n\}$ is an infinite sequence of independent identical distributed random variables with expectation $E(X_1),\cdots,E(X_n)=\mu$, we can have
\begin{equation*}
    \Bar{X}_n\rightarrow\mu, \text{when }n\rightarrow\infty,
\end{equation*}
where $\Bar{X}_n=\frac{1}{n}\sum^n_{i=1} X_i$.
\end{theorem}

\begin{remark}
We can approximate the expectation of an unknown distribution by performing a sufficient number of the trials.
\end{remark}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.15]{figure/1920px-Lawoflargenumbers.svg.png}
    \caption{Law of large numbers}
    % \label{fig:my_label}
\end{figure}

\begin{theorem}
\textbf{Central limit theorem(CLT).} In many situation, when independent random variables are added, their properly normalized sum tends toward a normal distribution even if the original variables themselves are not normal distributed.
\end{theorem}

\begin{remark}
The theorem is a key concept in probability theory because this theorem allows us to leverage the normal distribution to solve other unknown distributions.
\end{remark}

\begin{example}
Given a sequence of i.i.d. random variables $\{X_1,\cdots,X_n\}$, we randomly pick $m$(relatively large) samples from the population to calculate the mean value and repeat it $k$ times. And the $k$ mean values would be normal distributed.
\end{example}

\begin{definition}
\boxed{\textbf{Confidence interval}} is defined as 

\end{definition}

\begin{definition}
\boxed{\textbf{Covariance}} for two random variables $X$ and $Y$, each with sample size $d$ is defined by
\begin{align*}
    \operatorname{cov}(X,Y)&=E[(X-E[X])(Y-E(Y])]\\
    &=\sum^d_{i=1}\frac{(x_i-\Bar{x})(y_i-\Bar{y})}{d}
\end{align*}
\end{definition}

\begin{definition}
\boxed{\textbf{Covariance matrix}} for $n$ sets of random variables $\{X^1\},\cdots,\{X^n\}$, each with sample size $d$ is defined by
\begin{align*}
    \Sigma=
    \begin{pmatrix}
        \operatorname{cov}(X^1,X^1) & \cdots & \operatorname{cov}(X^1,X^n)\\
        \vdots &  \ddots &  \vdots \\
        \operatorname{cov}(X^n,X^1) & \cdots & \operatorname{cov}(X^n,X^n)
    \end{pmatrix}
\end{align*}
\end{definition}

\begin{definition}
\boxed{\textbf{Principal components analysis(PCA)}} seeks a sequence of linear subspaces that maximize the variance of the data, the intent of which is to find an orthonormal basis $\{e_1,\cdots,e_k\}$ of a set of points $\{S^1,\cdots,S^n\}\in\mathbb{R}^d$, and the new basis is of the same dimension $d$ as the original space, which satisfies the recursive relationship
\begin{align*}
    e_1&=\argmax_{\|e\|=1}\sum^n_{i=1}\langle e,S^i\rangle^2\\
    e_2&=\argmax_{\|e\|=1}\sum^n_{i=1}[\langle e_1,S^i\rangle^2+\langle e,S^i\rangle^2]\\
    \vdots\\
    e_k&=\argmax_{\|e\|=1}\sum^n_{i=1}[\langle e_1,S^i\rangle^2+\cdots+\langle e_{k-1},S^i\rangle^2+\langle e,S^i\rangle^2]
\end{align*}
In other words, the subspace $V_k=\operatorname{span}(\{e_1,\cdots,e_k\})$ is the $k$-dimensional subspace that maximizes the variance of the data projected to the subspace. The basis $\{e_i\}$ is computed as the set of ordered eigenvectors of the sample covariance matrix of the data.
\end{definition}

\begin{remark}
Concatenating all the feature vector together as a matrix, using SVD to decompose the matrix is a common way of performing PCA.
\end{remark}

\begin{example}
Given a dataset $S=\{S^1,\cdots,S^n\}$ of $n$ samples, where $S^i\in\mathbb{R}^d$ and $\Bar{S}$ is the mean of the dataset $S$
\begin{equation*}
    S^i=
    \begin{pmatrix}
        s^i_1\\
        \vdots\\
        s^i_d
    \end{pmatrix},
    \Bar{S}=
    \begin{pmatrix}
        \Bar{s}_1\\
        \vdots\\
        \Bar{s}_d
    \end{pmatrix}.
\end{equation*}
Then we can have a $d\times d$ covariance matrix $\Sigma$
\begin{equation*}
    \Sigma=
    \begin{pmatrix}
        \operatorname{cov}(s_1,s_1) & \cdots & \operatorname{cov}(s_1,s_d)\\
        \vdots &  \ddots &  \vdots \\
        \operatorname{cov}(s_d,s_1) & \cdots & \operatorname{cov}(s_d,s_d)
    \end{pmatrix}=
    \begin{pmatrix}
        \text{cov of $1^{st}$ and $1^{st}$ dim} & \cdots & \text{cov of $1^{st}$ and $d^{th}$ dim}\\
        \vdots &  \ddots &  \vdots \\
        \text{cov of $d^{th}$ and $1^{st}$ dim} & \cdots & \text{cov of $d^{th}$ and $d^{th}$ dim}
    \end{pmatrix},
\end{equation*}
where
\begin{equation*}
    \operatorname{cov}(s_j,s_k)=\frac{\sum^n_{i=1}(s^i_j-\Bar{s}_j)(s^i_k-\Bar{s}_k)}{n}.
\end{equation*}
To form a reduced dimension space $S'$, simply pick the largest $k$ eigenvalues of the covariance matrix above and the corresponding eigenvectors and the new dataset $S'$ is formed as below
\begin{equation*}
    \underbrace{
    \begin{pmatrix}
        S'^1 & \cdots & S'^n
    \end{pmatrix}}_{k\times n}=
    \underbrace{
    \begin{pmatrix}
        e^T_1\\
        \vdots\\
        e^T_p
    \end{pmatrix}}_{k\times d}\cdot
    \underbrace{
    \begin{pmatrix}
        S^1 & \cdots & S^n
    \end{pmatrix}}_{d\times n}
\end{equation*}
\end{example}

\begin{definition}
\boxed{\textbf{Autoencoder}} is a type of artificial neural network used to learn efficient data codings in an unsupervised(do not require labeled inputs to enable learning) manner. It is constituted by two main parts: an encoder that maps the input into the code(latent variable), and a decoder that maps the code(latent variable) to a reconstruction of the input.
\begin{align*}
    \phi &:{\mathcal {X}}\rightarrow {\mathcal {F}}\\
    \psi &:{\mathcal {F}}\rightarrow {\mathcal {X}}\\
    \phi ,\psi &={\underset {\phi ,\psi }{\operatorname {arg\,min} }}\,\|X-(\psi \circ \phi )X\|^{2} && \triangleright \text{loss function}
\end{align*}
\end{definition}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.35]{figure/origdata.png}
    \includegraphics[scale=0.35]{figure/pca.png}
    \includegraphics[scale=0.35]{figure/ae.png}
    \caption{From left to right: 2D original distribution, distribution recovered by PCA and distribution recovered by autoencoder. Since the original feature space is 2D, namely the covariance matrix of the distribution is of $2\times 2$, which means the distribution would be 1D after dimension reduction, and this is reason why the distribution recovered by PCA is totally linear.}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{figure/origdata3d.png}
    \includegraphics[scale=0.4]{figure/pca3d.png}
    \includegraphics[scale=0.4]{figure/ae3d.png}
    \caption{From left to right: 3D original distribution, distribution recovered by PCA and distribution recovered by autoencoder. Since the original feature space is 3D, which means the distribution would be 2D after dimension reduction, and this is reason why the distribution recovered by PCA looks like a plane.}
\end{figure}

\begin{remark}
Relationship between \textbf{PCA} and \textbf{autoencoder}\footnote{\url{https://github.com/muaz-urwa/PCA-vs-AutoEncoders/blob/master/PCAvsAE.ipynb}}:
\begin{itemize}
    \item PCA is essentially a linear transform method while autoencoders are capable of modeling non-linear complex functions.
    \item Features in PCA are linearly uncorrelated with each other, as they are orthogonal. While the autoencoded features may have correlations since they are just trained for accurate reconstruction.
    \item PCA is faster and computational cheap compared to autoencoder.
    \item Autoencoder is prone to overfitting due to large amount of parameters.
\end{itemize}
\end{remark}

\begin{example}
\texttt{PyTorch} implementation of autoencoder:
\begin{verbatim}
    class AutoEncoder(nn.Module):
    def __init__(self):
        super(AutoEncoder, self).__init__()

        self.encoder = nn.Sequential(
            nn.Linear(28*28, 128),
            nn.Tanh(),
            nn.Linear(128, 64),
            nn.Tanh(),
            nn.Linear(64, 12),
            nn.Tanh(),
            nn.Linear(12, 3),   # compress to 3 features which can be visualized in plt
        )
        self.decoder = nn.Sequential(
            nn.Linear(3, 12),
            nn.Tanh(),
            nn.Linear(12, 64),
            nn.Tanh(),
            nn.Linear(64, 128),
            nn.Tanh(),
            nn.Linear(128, 28*28),
            nn.Sigmoid(),       # compress to a range (0, 1)
        )

    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return encoded, decoded
\end{verbatim}
\end{example}

\begin{example}
Different types of autoencoders:
\begin{itemize}
    \item \textbf{Sparse autoencoder}\footnote{\url{https://web.stanford.edu/class/cs294a/sparseAutoencoder.pdf}} constraints most of the hidden units inactivive(close to 0) by adding a regularization term to the loss function
    \begin{align*}
        L(x,\hat{x})&+\lambda\sum_i\lVert a^{(h)}_i\rVert&&\triangleright\text{$L^1$ regularization}\\
        L(x,\hat{x})&+\sum_j D_{\operatorname{KL}}(\rho,\rho_j)&&\triangleright\text{KL divergence regularization}
    \end{align*}
    where $a^{(h)}_i$ is the activation output of $h$-th layer's hidden unit $i$, $\rho$ is the sparsity parameter which is typically preset as a small value close to 0 and $\rho_j$ denotes the average activation of hidden unit $j$(averaging over all training data).
    \item \textbf{Denoising autoencoder}'s input is the noised original data, and its output should be the noise-free original data.
\end{itemize}
\end{example}

\newpage
\section{Deep Learning}
\subsection{Common terms}
\begin{definition}
One \boxed{\textbf{batch}} of samples is a group of samples concatenated together to go through the network before updating the model weights.
\end{definition}

\begin{table}[H]
\centering
\begin{tabular}{cc}
\hline
{ \textbf{Types}}              & { \textbf{Batch size}}                    \\ \hline
Stochastic gradient descent & 1 \\
Mini-Batch gradient descent & $(1, \text{size of training dataset})$\\
Batch gradient descent      & size of training dataset \\ \hline
\end{tabular}
\end{table}

\begin{example}
In PyTorch's implementation, mini-batch concatenates the designated samples together and put it into the network. For instance, if the image is of size 128*128*128, 4 channels, and mini-batch size is 2, then the shape of input tensor during training is 2*4*128*128*128. 

You may wonder that when it comes to testing, the input is only one sample at a time, but the model stays the same, how can that still work? The things is, a model only regulates the shape of convolution kernel, pooling parameters, activation function and how they are organized, the size of the input does not matter, it can either 2*4*128*128*128 or 1*4*128*128*128. The final loss is actually summation of all samples' loss. As long as the output size matches the ground-truth size, the workflow can be performed uninterruptedly.
\end{example}

\begin{definition}
One \boxed{\textbf{iteration}} is a time span when an a batch of training data is passed forward and backward through the neural network.
\end{definition}

\begin{definition}
One \boxed{\textbf{epoch}} is a time span when an entire training dataset is passed forward and backward through the neural network.
\end{definition}

\begin{definition}
In \boxed{\textbf{convolutional layer}}, multiple channels of feature maps were extracted by sliding the trainable convolutional kernels across the input feature maps.
\end{definition}

\begin{definition}
\boxed{\textbf{Max pooling layer}} is a way to reduce the image sizes, to provide an abstracted form of the representation, to reduce the computational cost and to promote spatial invariance of the network.
\end{definition}

\begin{definition}
\boxed{\textbf{Fully-connected input layer}} takes the output of the previous layers, ``flattens'' them and turns them into a single vector that can be an input for the next stage.
\end{definition}

\begin{definition}
\boxed{\textbf{Dropout layer}} is used to alleviate data overfitting.
\end{definition}

\begin{table}[H]
\centering
\caption{VGGNet in detail}
\begin{tabular}{llll}
\hline
\multicolumn{1}{c}{\textbf{Layer}} & \multicolumn{1}{c}{\textbf{Size}} & \multicolumn{1}{c}{\textbf{Memory}} & \multicolumn{1}{c}{\textbf{Weights}} \\ \hline
INPUT                              & {[}224x224x3{]}                   & 224*224*3=150K                      & 0                        \\
CONV3-64                           & {[}224x224x64{]}                  & 224*224*64=3.2M                     & (3*3*3)*64               \\
CONV3-64                           & {[}224x224x64{]}                  & 224*224*64=3.2M                     & (3*3*64)*64              \\
POOL2                              & {[}112x112x64{]}                  & 112*112*64=800K                     & 0                        \\
CONV3-128                          & {[}112x112x128{]}                 & 112*112*128=1.6M                    & (3*3*64)*128             \\
CONV3-128                          & {[}112x112x128{]}                 & 112*112*128=1.6M                    & (3*3*128)*128            \\
POOL2                              & {[}56x56x128{]}                   & 56*56*128=400K                      & 0                        \\
CONV3-256                          & {[}56x56x256{]}                   & 56*56*256=800K                      & (3*3*128)*256            \\
CONV3-256                          & {[}56x56x256{]}                   & 56*56*256=800K                      & (3*3*256)*256            \\
CONV3-256                          & {[}56x56x256{]}                   & 56*56*256=800K                      & (3*3*256)*256            \\
POOL2                              & {[}28x28x256{]}                   & 28*28*256=200K                      & 0                        \\
CONV3-512                          & {[}28x28x512{]}                   & 28*28*512=400K                      & (3*3*256)*512            \\
CONV3-512                          & {[}28x28x512{]}                   & 28*28*512=400K                      & (3*3*256)*512            \\
CONV3-512                          & {[}28x28x512{]}                   & 28*28*512=400K                      & (3*3*256)*512            \\
POOL2                              & {[}14x14x512{]}                   & 14*14*512=100K                      & 0                        \\
CONV3-512                          & {[}14x14x512{]}                   & 14*14*512=100K                      & (3*3*512)*512            \\
CONV3-512                          & {[}14x14x512{]}                   & 14*14*512=100K                      & (3*3*512)*512            \\
CONV3-512                          & {[}14x14x512{]}                   & 14*14*512=100K                      & (3*3*512)*512            \\
POOL2                              & {[}7x7x512{]}                     & 7*7*512=25K                         & 0                        \\
FC                                 & {[}1x1x4096{]}                    & 4096                                & 7*7*512*4096             \\
FC                                 & {[}1x1x4096{]}                    & 4096                                & 4096*4096                \\
FC                                 & {[}1x1x1000{]}                    & 1000                                & 4096*1000                \\ \hline
\end{tabular}
\end{table}

% \subsection{\texttt{PyTorch} functions}

\subsection{Loss Functions}
\begin{definition}
\boxed{\textbf{Intersection of Union(Jaccard index, IoU)}} is defined as 
\begin{equation*}
    \operatorname{IoU}=\frac{|A\cap B|}{|A\cup B|}
\end{equation*}
\end{definition}

\begin{definition}
\boxed{\textbf{Dice loss function}} is defined as 
\begin{equation*}
    L_{\operatorname{dice}}=-\frac{2|A\cap B|}{|A|+|B|}=-\frac{2\times\left<v_{\operatorname{true}}, v_{\operatorname{pred}}\right>}{\|v_{\operatorname{true}}\|^2_2+\|v_{\operatorname{pred}}\|^2_2+\epsilon}\in(-1,0],
\end{equation*}
where $v_{\operatorname{true}},v_{\operatorname{pred}}\in\mathbb{R}^{h\times w\times d}$ are \textbf{one-hot vectors}, and $\epsilon$ is a small constant to avoid zero division. 
\end{definition}

\begin{remark}
Since dice loss function is a loss function, just as other loss function, lower score indicates better alignment. And that's the reason why the negative sign is indispensable.
\end{remark}

\begin{definition}
\boxed{\textbf{$L^1$ loss function}} is defined as 
\begin{align*}
    L_{L^1}&=\|v_{\operatorname{true}}-v_{\operatorname{pred}}\|_1,\\
    &=\sum _{i=1}^{n}|v_i^{\operatorname{pred}}-v_i^{\operatorname{true}}|,
\end{align*}
where $v_{\operatorname{true}},v_{\operatorname{pred}}$ are vectors.
\end{definition}

\begin{definition}
\boxed{\textbf{$L^2$ loss function}} is defined as 
\begin{align*}
    L_{L^2}&=\|v_{\operatorname{true}}-v_{\operatorname{pred}}\|^2_2,\\
    &=\sqrt{\sum _{i=1}^{n}(v_i^{\operatorname{pred}}-v_i^{\operatorname{true}})^{2}}^2,\\
    &=\sum _{i=1}^{n}(v_i^{\operatorname{pred}}-v_i^{\operatorname{true}})^{2},
\end{align*}
where $v_{\operatorname{true}},v_{\operatorname{pred}}$ are vectors.
\end{definition}

\begin{definition}
\boxed{\textbf{Mean squared error(MSE) loss function}} is defined as 
\begin{equation*}
    L_{\operatorname {MSE}} ={\frac {1}{n}}\sum _{i=1}^{n}(v_i^{\operatorname{pred}}-v_i^{\operatorname{true}})^{2}.
\end{equation*}
\end{definition}

\begin{remark}
% Basically, the  is the squared , while the MSE loss is the averaged $L^2$ loss.
\begin{equation*}
    L^2\text{ norm}\xrightarrow{squared}L^2\text{ loss}\xrightarrow{averaged}\text{MSE loss}
\end{equation*}
\end{remark}

\begin{remark}
Do not confuse mean squared error with least squares function, which is used in linear regression.
\end{remark}

\begin{definition}
\boxed{\textbf{Hausdorff distance}} is defined as 
\begin{equation*}
     d_{\mathrm {H} }(X,Y)=\max \left\{\,\sup _{x\in X}\inf _{y\in Y}d(x,y),\,\sup _{y\in Y}\inf _{x\in X}d(x,y)\,\right\}
\end{equation*}
where X and Y be two non-empty subsets of a metric space $(M,d)$.
\end{definition}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{figure/hausdorff.png}
    \caption{Components of the calculation of the Hausdorff distance between the green line X and the blue line Y.}
\end{figure}

\begin{definition}
\boxed{\textbf{Entropy}} of the \textbf{discrete probability distribution} $p$ is defined as
\begin{equation*}
   H(p)=-\sum _{x\in {\mathcal {X}}}{p (x)\log p (x)} \in[0,1]
\end{equation*}

\begin{remark}
If entropy is closer to 1, means the distribution is of high level of disorder (low level of purity). If entropy is closer to 0, means the distribution is of low level of disorder (high level of purity).
\end{remark}

\begin{definition}
\boxed{\textbf{Cross-entropy loss function}} of the \textbf{discrete probability distribution} $p$ and $q$ over a given set is defined as 
\begin{equation*}
    H(p,q)=-\sum_{x\in {\mathcal {X}}} p(x)\log(q(x))=-\left<p,\log(q)\right>\in[0,\infty),
\end{equation*}
which measures the distance between the distributions.
\end{definition}

\begin{remark}
Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a \textbf{probability value between 0 and 1}. Cross-entropy loss increases as the predicted probability diverges from the actual label. 
\end{remark}

\end{definition}

\begin{example}
As for the two discrete distributions below:
\begin{table}[H]
\centering
\begin{tabular}{cccc}
\hline
\textbf{Distribution} & \textbf{Class A} & \textbf{Class B} & \textbf{Class C} \\ \hline
$p$            & 0       & 0       & 1    \\ 
$q$           & 0.1     & 0.2     & 0.7        \\ \hline
\end{tabular}
\end{table}
we can have the cross entropy 
\begin{equation*}
    H(p,q)=-(0\times\log(0.1)+0\times\log(0.2)+1\times\log(0.7))=0.15
\end{equation*}
\end{example}

\begin{remark}
According to the example above, the only thing that contributes to the value of loss is the predicted possibility of the ground-truth class, since the possibilities of other classes are all wiped out by 0. The closer predicted possibility of the ground-truth class to 1, the lower loss will be.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.25]{figure/log.jpg}
    \caption{$y=-\log(x)$}
    % \label{fig:my_label}
\end{figure}
\end{remark}

\begin{definition}
\boxed{\textbf{Kullback–Leibler(KL) divergence}} is defined as
\begin{align*}
    D_{\text{KL}}(p,q)&=\sum _{x\in {\mathcal {X}}}p(x)\log \left({\frac {p(x)}{q(x)}}\right)\\
    &=-\sum _{x\in {\mathcal {X}}}p(x)\log q(x)+\sum _{x\in {\mathcal {X}}}p(x)\log p(x)\\
    &=-H(p,q)+H(p)\in[0,\infty)\\
    &=\operatorname{entropy}-\text{cross entropy},
\end{align*}
\end{definition}

\begin{remark}
KL divergence describes how different $q$ is from $p$ from the perspective of $p$. When $p$ is fixed, just as the ground-truth data in training, minimizing KL divergence and cross-entropy is equivalent, as $H(p)$ is a constant.
\end{remark}

\begin{example}
As for the two discrete distributions below:
\begin{table}[H]
\centering
\begin{tabular}{cccc}
\hline
\textbf{Distribution} & \textbf{Class A} & \textbf{Class B} & \textbf{Class C} \\ \hline
$p$            & 0       & 0       & 1    \\ 
$q_1$           & 0.1     & 0.2     & 0.7        \\ 
$q_2$           & $1-1^{-\infty}$     & 0     & $1^{-\infty}$        \\ \hline
\end{tabular}
\end{table}
we can have the KL divergence
\begin{equation*}
    D_{\operatorname{KL}}(p,q_1)=(0\times\log(0)+0\times\log(0)+1\times\log(1))-(0\times\log(0.1)+0\times\log(0.2)+1\times\log(0.7))=0.15
\end{equation*}
When the two distribution are very far away just like $p,q_2$, $D_{\operatorname{KL}}(p,q_2)=\infty$, which is meaningless.
\end{example}

\begin{definition}
\boxed{\textbf{Jensen–Shannon(JS) divergence}} is defined as
\begin{align*}
    D_{\text{JS}}(p,q)&=\frac{1}{2}D_{\text{KL}}(p,\frac{1}{2}(p+q))+\frac{1}{2}D_{\text{KL}}(q,\frac{1}{2}(p+q))
\end{align*}
\end{definition}

\begin{definition}
\boxed{\textbf{Wasserstein distance}} is defined as 
\begin{align*}
    W_{p}(\mu ,\nu )&=\left(\inf _{\gamma \in \Gamma (\mu ,\nu )}\int _{M\times M}\mathrm{dist}(x,y)^{p}\,\mathrm {d} \gamma (x,y)\right)^{1/p},\\
    &=\left(\inf _{\gamma \in \Gamma (\mu ,\nu )}\int _{M\times M}\mathrm{dist}(x,y)^{p}\gamma (x,y)\mathrm {d}x\mathrm {d}y\right)^{1/p},\\
    W_{2}(\mu ,\nu )&=\left(\inf _{\gamma \in \Gamma (\mu ,\nu )}\int _{M\times M}\mathrm{dist}(x,y)^{2}\gamma (x,y)\mathrm {d}x\mathrm {d}y\right)^{1/2},
\end{align*}
where $\Gamma(\mu,\nu)$ denotes the set of all coupling of marginal distribution $\mu$ and $\nu$, and $x, y$ are actually indicating the position in the respective distribution.

In discrete case, the distance reads as
\begin{equation*}
    W_{p}(\mu ,\nu )=\min_{T \in \Gamma (\mu ,\nu )}\langle T,M_{\mu\nu}\rangle=\min_{T \in \Gamma (\mu ,\nu )}\operatorname{tr}(T^TM_{\mu\nu}),
\end{equation*}
where $M_{\mu\nu}=[\operatorname{dist}(x_i,y_j)^p]_{ij}\in\mathbb{R}^{m\times n}$ and $\Gamma(\mu,\nu)=\{T\in\mathbb{R}^{m\times n}_{+}|T\mathbbm{1}_m=a,T^T\mathbbm{1}_n=b\}$, namely the transport map matrix $T$ is under the constraint that the sum of each row and column equals to $a$ and $b$, respectively. The two matrices correspond to $\operatorname{dist}(x,y)$ and $\gamma(x,y)$ in continuous form.
\end{definition}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.15]{figure/Transport-plan.png}
    \caption{Wasserstein distance, credits to Wikipedia}
\end{figure}

\begin{remark}
Wasserstein distance is closely related to optimal transport problem. That is, for two distributions of mass $\mu(x),\nu(y)$ in the space $S$, where $x,y\in S$, we wish to transport the mass at the lowest cost. The problem only makes sense when the sums of two distributions are identical, fortunately $\mu(x),\nu(y)$ are two probability distributions, namely the sum of mass equals to 1, this premise will be satisfied.

Assuming there is also a cost function $c(x,y)\rightarrow[0,+\infty)$ which indicates the cost of transporting \textbf{unit mass} from point $x$ to point $y$. Function $\gamma(x,y)$ depicts a transport plan which gives the amount of mass moved from point $x$ to point $y$. 
% In order to make this plan meaningful, it should satisfy the following properties
% \begin{align*}
%     \int\gamma(x,y)dx=\nu(y)\\
%     \int\gamma(x,y)dy=\mu(x)
% \end{align*}
Therefore, the cost of the whole transport plan equals to
\begin{equation*}
    \int\int c(x,y)\gamma(x,y)dxdy,
\end{equation*}
and the Wasserstein distance is exactly the cost of optimal transport plan.
\end{remark}

\begin{lemma}
The $p$-Wasserstein distance between the two probability measures $\mu$ and $\nu$ on $\mathbb{R}^1$ has the following closed-form expression:
\begin{align*}
    W_{p}(\mu,\nu)&=\left(\int^{+\infty}_{-\infty}|U(s)-V(s)|^{p}ds\right)^{1/p}&&\triangleright\int\text{quantity}\times\text{unit distance}\\
    &=\left(\int^1_0|U^{-1}(t)-V^{-1}(t)|^{p}dt\right)^{1/p},&&\triangleright\int\text{distance}\times\text{unit quantity}
\end{align*}
where $U$ and $V$ are the CDFs of $\mu,\nu$ respectively.
\begin{proof}
Assuming $\{(x_1,y_1), (x_2,y_2)\}\subset \supp(\gamma^*)\footnote{The support of a probability distribution can be loosely thought of as the closure of the set of possible values of a random variable having that distribution. }, x_1<x_2$, where $\gamma^*$ denotes the optimal transport plan. Given the previous assumption, we claim $y_1\le y_2$.

Supposing that $y_1\le y_2$ is not the case, namely $y_1>y_2$, which yields\footnote{For a more detailed deviation of this inequality in the case of $p>1$, refers to Appendix A in \url{https://arxiv.org/pdf/1509.02237.pdf}} 
\begin{equation*}
    |x_1-y_2|^p+|x_2-y_1|^p<|x_1-y_1|^p+|x_2-y_2|^p
\end{equation*}
However this inequality suggests that $\{(x_1,y_2), (x_2,y_1)\}\subset \supp(\gamma^*)$, rather than $\{(x_1,y_1), (x_2,y_2)\}\subset \supp(\gamma^*)$, which contradicts the initial assumption, namely the optimality of $\gamma^*$, as it indicates that $\gamma^*$ is no cyclically monotone.

Now, for $x\in\supp(\mu), y\in\supp(\nu)$, we claim that $(x,y)\in\supp(\gamma^*)$ if and only if $U(x)=V(y)$. To see this, note that form the monotonicity property we just built, we deduce that $(x,y)\in\supp(\gamma^*)$ if and only if
\begin{equation*}
    \gamma^*(\mathbb{R},(-\infty,y])=\gamma^*((-\infty,x],(-\infty,y])=\gamma^*((-\infty,x],\mathbb{R})
\end{equation*}
In turn, the fact that $\gamma^*\in\Gamma(\mu,\nu)$ implies that $\gamma^*((-\infty,x],\mathbb{R})=F(x)$ and $\gamma^*(\mathbb{R},(-\infty,y])=G(y)$. From previous relation, we conclude that
\begin{equation*}
    W_{p}(\mu ,\nu )=\left(\inf _{\gamma \in \Gamma (\mu ,\nu )}\int _{M\times M}\mathrm{dist}(x,y)^{p}\gamma (x,y)\mathrm {d}x\mathrm {d}y\right)^{1/p}=\left(\int^1_0|F^{-1}(t)-G^{-1}(t)|^{p}dt\right)^{1/p}
\end{equation*}
\end{proof}
\end{lemma}

\begin{example}
For one dimensional discrete case, to transport $\nu$ to $\mu$, 
\begin{enumerate}
    \item 4 extra squares would be moved from 0 to 1;
    \item 3 extra squares would be moved from 1 to 2;
    \item 2 extra squares would be moved from 2 to 3;
    \item 1 extra squares would be moved from 3 to 4.
\end{enumerate}
The ``earth'' need to be moved is exactly the difference between the two CDFs at each location. Therefore the $p$-Wasserstein distance equals to $(\sum|U(s)-V(s)|^{p}ds)^{1/p}=(\sum4^p\times1+3^p\times1+2^p\times1+1^p\times1)^{1/p}$
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.2]{figure/mupdf.png}
    \includegraphics[scale=0.2]{figure/nupdf.png}
    \includegraphics[scale=0.2]{figure/mucdf.png}
    \includegraphics[scale=0.2]{figure/nucdf.png}
    \caption{Two distribution $\mu$ and $\nu$.}
    \label{fig:my_label}
\end{figure}
\end{example}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.3]{figure/ot.png}
    \caption{Optimal transport measures the minimal effort required for filling $−µ_1$ with $µ_0$, i.e. transporting one distribution to another.}
\end{figure}

\begin{remark}
Relationship between \textbf{KL divergence}, \textbf{JS divergence} and \textbf{Wasserstein distance}:
\begin{itemize}
    \item Intuitively, KL divergence looks like a distance between two distributions, however $D_{\text{KL}}(p,q)\not=D_{\text{KL}}(q,p)$, namely it is asymmetric. So comes the JS divergence.
    \item When the two distributions are far apart, the KL divergence cannot reflect the distance between the distributions while JS divergence is constant, which is deadly for backpropagation in neural network. Nevertheless, the Wasserstein distance can tackle this drawback of KL/JS divergence, as the optimal transport plan of two distant distributions would always make sense and variable.
\end{itemize}
\end{remark}

\begin{remark}
Advantages of Wasserstein distance:\footnote{Figures in this remark credit to \url{https://www.stat.cmu.edu/~larry/=sml/Opt.pdf}}
\begin{itemize}
    \item By leveraging Wasserstein distance, we can get a better average/summary image of two distribution.
    \begin{figure}[H]
        \centering
        \includegraphics[scale=0.2]{figure/average.png}
        \caption{ Top: Some random circles. Bottom left: Euclidean average of the circles. Bottom right: Wasserstein barycenter.}
    \end{figure}
    \item When we are creating a geodesic between two distributions $P_0, P_1$, and $P_t$ interpolates between them, Wasserstein distance can preserve the basic structure of the distribution.
    \begin{figure}[H]
        \centering
        \includegraphics[scale=0.2]{figure/distributiongeo.png}
        \caption{Top row: Geodesic path from $P_0$ to $P_1$. Bottom row: Euclidean path($P_t=tP_0+(1-t)P_1$) from $P_0$ to $P_1$.}
    \end{figure}
    \item Wasserstein distance is insensitive to small wiggles.
\end{itemize}
\end{remark}

\subsection{Normalization}
Normalization is used to reduce internal covariate shift among the training samples.
\begin{definition}
\boxed{\textbf{Batch normalization}}\cite{batch} is defined as
\begin{align*}
     \mu_{c}&=\frac{1}{HWN}\sum^N_n\sum^W_w\sum^H_h x_{cnhw}\\
     \sigma_{c}^2&=\frac{1}{HWN}\sum^N_n\sum^W_w\sum^H_h(x_{cnhw}-\mu_c)^2\\
     \hat{x}_{c}&=\frac{x_{c}-\mu_{c}}{\sqrt{\sigma_{c}^2+\epsilon}}
\end{align*}
where $x$ are the values of input over a mini-batch, $C,N,H,W$ are the channel, batch, height, width size, respectively. Batch normalization should be performed by each channel.
\end{definition}

\begin{definition}
\boxed{\textbf{Layer normalization}} is defined as
\begin{align*}
     \mu_{n}&=\frac{1}{HWC}\sum^C_c\sum^W_w\sum^H_h x_{cnhw}\\
     \sigma_{n}^2&=\frac{1}{HWC}\sum^C_c\sum^W_w\sum^H_h(x_{cnhw}-\mu_n)^2\\
     \hat{x}_{n}&=\frac{x_{n}-\mu_{n}}{\sqrt{\sigma_{n}^2+\epsilon}}
\end{align*}
where $x$ are the values of input over a mini-batch, $C,N,H,W$ are the channel, batch, height, width size, respectively. Layer normalization should be performed by each sample in a mini-batch.
\end{definition}

\begin{definition}
\boxed{\textbf{Instance normalization}} is defined as
\begin{align*}
     \mu_{cn}&=\frac{1}{HW}\sum^W_w\sum^H_h x_{cnhw}\\
     \sigma_{cn}^2&=\frac{1}{HW}\sum^W_w\sum^H_h(x_{cnhw}-\mu_{cn})^2\\
     \hat{x}_{cn}&=\frac{x_{cn}-\mu_{cn}}{\sqrt{\sigma_{cn}^2+\epsilon}}
\end{align*}
where $x$ are the values of input over a mini-batch, $C,N,H,W$ are the channel, batch, height, width size, respectively. Instance normalization should be performed by each channel and sample in a batch.
\end{definition}

\begin{definition}
\boxed{\textbf{Group normalization}}\cite{group} is a middle ground between layer and instance normalization, which is defined as
\begin{align*}
     \mu_{gn}&=\frac{1}{HW|G_i|}\sum^{|G_i|}_{c\in G_i}\sum^W_w\sum^H_h x_{cnhw}\\
     \sigma_{gn}^2&=\frac{1}{HW|G_i|}\sum^{|G_i|}_{c\in G_i}\sum^W_w\sum^H_h(x_{cnhw}-\mu_n)^2\\
     \hat{x}_{gn}&=\frac{x_{n}-\mu_{n}}{\sqrt{\sigma_{n}^2+\epsilon}}\\
     G&=\{G_1,\cdots\}\\
     G_i&=\{c_i,\cdots,c_{i'}\}
\end{align*}
where $x$ are the values of input over a mini-batch, $C,N,H,W,|G|$ are the channel, batch, height, width, group size, respectively. Instance normalization should be performed by each group among the channel set.
\end{definition}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{figure/normalization.png}
    \caption{Normalization methods. Each subplot shows a feature map tensor, with $N$ as the batch axis, $C$ as the channel axis, and $(H, W)$ as the spatial axes. The pixels in blue are normalized by the same mean and variance, computed by aggregating the values of these pixels.}
    % \label{fig:my_label}
\end{figure}

% \subsection{Optimizer}
% \begin{definition}
% \boxed{\textbf{Adam optimizer}}
% \end{definition}
% \subsection{Regularization}
\subsection{Activation Function}
\begin{table}[H]
\centering
\begin{tabular}{cccc}
\hline
\textbf{Types}                    & \textbf{Functions}                                       & \textbf{Range}                                           & \textbf{Order of continuity} \\ \hline
Sigmoid(logistic)        & $\sigma(x)=\frac {1}{1+e^{-x}}$               & $(0,1)$  & $C^\infty$\\
Tanh(hyperbolic tangent) & $\tanh(x)={\frac {e^{x}-e^{-x}}{e^{x}+e^{-x}}}$ & $(-1,1)$   & $C^\infty$\\ \hline
ReLU(rectified linear unit)  & $\operatorname{ReLU}(x)=\max\{0,x\}$            & $[0,+\infty)$    & $C^0$\\       
Leaky ReLU               & $\operatorname{Leaky ReLU}(x)=\max\{0.01x,x\}$  & $(-\infty,+\infty)$ & $C^0$\\
ELU(exponential linear unit)& ${\begin{cases}\alpha \left(e^{x}-1\right)&{\text{if }}x\leq 0\\x&{\text{if }}x>0\end{cases}}$ & $(-\alpha,+\infty)$ & $\begin{cases}C^{1}&{\text{if }}\alpha =1\\C^{0}&{\text{otherwise}}\end{cases}$\\ 
GELU(gaussian error linear unit)& $\begin{aligned}&{\frac {1}{2}}x\left(1+{\text{erf}}\left({\frac {x}{\sqrt {2}}}\right)\right)\end{aligned}$ & $(-0.17,+\infty)$ & $C^\infty$\\
Smooth ReLU(Softplus) & $\ln(1+e^{x})$ &$(0,+\infty)$ & $C^\infty$\\
\hline
\end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{figure/sigmoid.png}
    \includegraphics[scale=0.6]{figure/tanh.png}
    \includegraphics[scale=0.6]{figure/relu.png}
    \includegraphics[scale=0.6]{figure/leakyrelu.png}
    \includegraphics[scale=0.6]{figure/elu.png}
    \includegraphics[scale=0.08]{figure/gelu.png}
    \includegraphics[scale=0.11]{figure/softplus.png}
    \caption{Plots of sigmoid, tahn, ReLU, Leaky ReLU, ELU, GELU, softplus activation functions, correspondingly.}
\end{figure}

The following table lists activation functions that are not functions of a single fold $x$ from the previous layer or layers:
\begin{table}[H]
\centering
\begin{tabular}{cccc}
\hline
{ \textbf{Types}} & { \textbf{Functions}}                            & { \textbf{Range}}        & { \textbf{Order of continuity}} \\ \hline
{ Softmax}        & { $\frac {e^{x_{i}}}{\sum _{j=1}^{J}e^{x_{j}}}$} & { $(0,1)$}               & { $C^{\infty }$}                \\ 
{ Maxout}         & { $\max _{i}x_{i}$}                              & { $ (-\infty ,\infty )$} & { $C^{0}$}                      \\ \hline
\end{tabular}
\end{table}

\begin{remark}
Activation functions have different mathematical properties:
\begin{itemize}
    \item \textbf{Nonlinear.} When the activation function is non-linear, then a two-layer neural network can be proven to be a universal function approximator. This is known as the Universal Approximation Theorem. The identity activation function does not satisfy this property. When multiple layers use the identity activation function, the entire network is equivalent to a single-layer model.
    \item \textbf{Range.} When the range of the activation function is finite, gradient-based training methods tend to be more stable, because pattern presentations significantly affect only limited weights. When the range is infinite, training is generally more efficient because pattern presentations significantly affect most of the weights. In the latter case, smaller learning rates are typically necessary.
    \item \textbf{Continuously differentiable.} This property is desirable (ReLU is not continuously differentiable and has some issues with gradient-based optimization, but it is still possible) for enabling gradient-based optimization methods. The binary step activation function is not differentiable at 0, and it differentiates to 0 for all other values, so gradient-based methods can make no progress with it.
    \item \textbf{Monotonic.} When the activation function is monotonic, the error surface associated with a single-layer model is guaranteed to be convex.
\end{itemize}
\end{remark}
% \subsection{Data Augmentation}
\section{Applications}
\subsection{Deep Learning for Solving Partial Differential Equation}
\paragraph{Overview.} When we are talking about a deep neural network, the first thing that comes to my mind is that the input/output of the network can be a image. However, in this chapter, we will learn that the neural network can also be a ``image'', in view of the function/mapping nature of a network, whose input and output are the coordinates and pixel value, respectively.

\begin{definition}
\boxed{\textbf{Ordinary differential equation}} is a differential equation containing one or more functions of one independent variable and the derivatives of those functions.
\begin{equation*}
    a_{0}(x)y+a_{1}(x)y'+a_{2}(x)y''+\cdots +a_{n}(x)y^{(n)}+b(x)=0
\end{equation*}
where $a_{0}(x), ..., a_{n}(x)$ and $b(x)$ are arbitrary differentiable functions that do not need to be linear, and $y',\ldots ,y^{(n)}$ are the successive derivatives of the unknown function $y$ of the variable $x$.
\end{definition}

\begin{example}
Integrating a curve from a vector field is actually solving the very naive ODE: $\frac{dy}{dt}=v(y,t)$, typically we solve this by Euler method. Solving ODE is not always as simple as integrating.
\end{example}

\begin{definition}
\boxed{\textbf{Partial differential equation}} is an equation which imposes relations between the various partial derivatives of a multivariable function. For example:
\begin{equation*}
    Au_{xx}+2Bu_{xy}+Cu_{yy}+\cdots {\mbox{(lower order terms)}}=0,
\end{equation*}
where the coefficients $A, B, C\cdots$ may depend upon $x$ and $y$.
\end{definition}

\begin{remark}
Relationship between ODE and PDE:
\begin{itemize}
    \item The differential operation in ODE is only with respect to one variable(not necessarily means the function only has one variable).
    \item The differential operation in PDE would be related to multiple variable.
\end{itemize}
\end{remark}

\begin{example}
When the equation involves a gradient $\nabla$ or Laplacian $\Delta$, it is a partial differential equation.
\end{example}

\subsubsection{PINN}\footnote{\url{https://www.osti.gov/servlets/purl/1595805}}
In one space dimension, the Burger’s equation along with Dirichlet boundary conditions reads as
\begin{align*}
    u_t+uu_x-\frac{0.01}{\pi}u_{xx}=0 && x\in[-1,1], t\in[0,1]\\
    u(0,x)=-\sin(\pi x)\\
    u(t,-1)=u(t,1)=0
\end{align*}
where $u(t,x):\mathbb{R}^2\rightarrow\mathbb{R}$. Let us define $f(t,x):\mathbb{R}^2\rightarrow\mathbb{R}$ to be given by
\begin{equation*}
    f(t,x)=u_t+uu_x-\frac{0.01}{\pi}u_{xx}
\end{equation*}
followed by approximating $u(t,x)$ by a deep neural network. Following is a \texttt{TensorFlow} snippet which highlight the simplicity of this idea:
\begin{verbatim}
    def u(t, x):
        u = neural_net(tf.concat([t,x],1), weights, biases)
        return u
        
    def f(t, x):
        u = u(t, x)
        u_t = tf.gradients(u, t)[0]
        u_x = tf.gradients(u, x)[0]
        u_xx = tf.gradients(u_x, x)[0]
        f = u_t + u*u_x - (0.01/tf.pi)*u_xx
        return f
\end{verbatim}

The shared parameters between the neural networks $u(t, x)$ and $f(t, x)$ can be learned by minimizing the mean squared error loss
\begin{align*}
    \operatorname{Loss}(\theta)&=\operatorname{MSE}_u+\operatorname{MSE}_f\\
    \operatorname{MSE}_u&=\frac{1}{N_u}\sum^{N_u}_{i=1}|u(t^i_u,x^i_u)-u^i|^2\\
    \operatorname{MSE}_f&=\frac{1}{N_u}\sum^{N_u}_{i=1}|f(t^i_u,x^i_u)-0|^2
\end{align*}
Here, $\{t^i_u,x^i_u,u^i\}^{N_u}_{i=1}$ denotes the initial and boundary(cross mark in Figure~\ref{fig:pde}) training data on $u(t,x)$ and $\{t^i_f,x^i_f\}^{N_f}_{i=1}$ specify the collocations points for $f(t,x)$.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{figure/pde.jpg}
    \caption{Solved two variables function: $u(t,x)$}
    \label{fig:pde}
\end{figure}

\begin{remark}
Neural network presents function $u$ in a \textbf{map}($\mathbb{R}^2\rightarrow\mathbb{R}$) manner, which requires a input and an output, while Figure~\ref{fig:pde} presents $u$ in a \textbf{image} manner. Thereby, the neural network can get myriad samples for training process, as the cross marks in figure above show. For the multi-variable functions($\mathbb{R}^n\rightarrow\mathbb{R}, n\le3$), we can regard the deep neural network for solving PDE as a $n$ dimensional \textbf{``image''}.
\end{remark}

\begin{example}
The 2D Poisson’s equation reads as
\begin{equation*}
    \nabla u=f
\end{equation*}
Consequently we can have the code snippet as below
\begin{verbatim}
    def u(x, y):
        u = neural_net(tf.concat([x,y],1), weights, biases)
        return u
        
    def f(x, y):
        u = u(x, y)
        u_x = tf.gradients(u, x)[0]
        u_xx = tf.gradients(u_x, x)[0]
        u_y = tf.gradients(u, y)[0]
        u_yy = tf.gradients(u_y, y)[0]
        f = u_xx + u_yy
        return f
\end{verbatim}
You can regard function $u(x,y), f(x,y)$ happen to be meaningful images, while the neural network is another way to present this function or image.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.2]{figure/camerau.png}
    \includegraphics[scale=0.2]{figure/cameraf.png}
    \caption{The recovered function $u(x,y)$ is a meaningful image.}
\end{figure}
\end{example}

\newpage
\begin{thebibliography}{99} 
\bibitem{batch}Ioffe, S. and Szegedy, C., 2015, June. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning (pp. 448-456). PMLR.
\bibitem{group}Wu, Y. and He, K., 2018. Group normalization. In Proceedings of the European conference on computer vision (ECCV) (pp. 3-19).
\end{thebibliography}
\end{document}