\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath,amsthm,enumitem}
\usepackage{graphicx}
\usepackage{lscape}
\usepackage{geometry}
\usepackage{color}
\usepackage{float}
\usepackage{listings}
\usepackage{amssymb}
\usepackage{verbatim}
\usepackage{algpseudocode} 
\usepackage{algorithm}  
\usepackage{algorithmicx}  
\usepackage{hyperref}
\usepackage{float}
\renewcommand{\baselinestretch}{1.5}
\usepackage{todonotes}
\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm} 

\title{Note on Machine Learning}
\author{Haocheng Dai}
\date{}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
% \newtheorem{definition}{Example}

\algnewcommand{\Inputs}[1]{%
  \State \textbf{Inputs:}\hspace*{\algorithmicindent}\parbox[t]{.8\linewidth}{\raggedright #1}
}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\newtheorem{corollary}{Corollary}
\newenvironment{example}[1][]{\refstepcounter{example}\par\medskip
   \noindent \textbf{Example~\theexample. #1} \rmfamily}{\medskip}
\newcounter{example}[Example]
      
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\begin{document}
\maketitle

\section{Statistics}
\begin{definition}
\boxed{\textbf{Likelihood}} is the probability of certain observation given the reason, namely
\begin{equation*}
    p(\text{observation}|\text{reason}).
\end{equation*}
\end{definition}

\begin{definition}
\boxed{\textbf{Prior probability}} is the probability of the reason without any observation, namely
\begin{equation*}
    p(\text{reason}).
\end{equation*}
\end{definition}

\begin{definition}
\boxed{\textbf{Posterior probability}} is the probability of the reason with certain observation, namely
\begin{align*}
    p(\text{reason}|\text{observation})&=\frac{p(\text{observation}|\text{reason})}{p(\text{observation})}\times p(\text{reason})\\
    &\propto \text{Likelihood}\times\text{Prior Probability}
\end{align*}
\end{definition}

\begin{definition}
\boxed{\textbf{Probability mass function(PMF)}} is a discrete function $f(x)$ that gives the probability that a discrete random variable is exactly equal to some value. The probability is acquired through
\begin{equation*}
    P(X=a)=f(a).
\end{equation*}
\end{definition}

\begin{definition}
\boxed{\textbf{Probability density function(PDF)}} is a continuous function $f(x)$ the gives the probability of a continuous random variable is located among certain interval. The probability is acquired through
\begin{equation*}
    P(a\leq X\leq b)=\int _{a}^{b}f(x)\,dx.
\end{equation*}
\end{definition}

\begin{example}
Suppose bacteria of a certain species typically live 4 to 6 hours. The probability that a bacterium lives exactly 5 hours is equal to zero. A lot of bacteria live for approximately 5 hours, but there is no chance that any given bacterium dies at exactly 5.0000000000... hours. However, the probability that the bacterium dies between 5 hours and 5.01 hours is quantifiable. 
\end{example}

\begin{remark}
A PDF must be integrated over an interval to yield a probability. 
\end{remark}

\begin{definition}
\boxed{\textbf{Cumulative density function(CDF)}} is a continuous function $F(x)$ the gives the probability of a continuous random variable is less than or equal to $x$.
\begin{align*}
    F(x)&=\int ^{x}_{-\infty}f(t)\,dt.\\
    P(X\leq b)&=F(b)
\end{align*}
\end{definition}

\begin{remark}
PDF is the derivative of the CDF.
\end{remark}

\begin{definition}
\boxed{\textbf{Maximum likelihood estimation(MLE)}} is a method of estimating the parameters of a probability distribution by maximizing a likelihood function, so that under the assumed statistical model the observed data is most probable.
\end{definition}

\begin{example}
Now suppose that there was only one coin but its $p$ could have been any value $0\le p\le 1$. The likelihood function to be maximized is
\begin{equation*}
    L(p)={\binom {80}{49}}p^{49}(1-p)^{31}
\end{equation*}
and the maximization is over all possible values $0\le p\le 1$.

By differentiating $L(p)$ with respect to $p$ and setting to zero, we yield that the maximum likelihood estimator for p is $\frac{49}{80}$.
\end{example}

\begin{definition}
\boxed{\textbf{Monte Carlo methods}} are a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results. Monte Carlo methods vary, but tend to follow a particular pattern:
\begin{enumerate}
    \item Define a domain of possible inputs;
    \item Generate inputs randomly from a probability distribution over the domain;
    \item Perform a deterministic computation on the inputs;
    \item Aggregate the results.
\end{enumerate}
\end{definition}

\begin{example}
Consider a quadrant (circular sector) inscribed in a unit square. Given that the ratio of their areas is $\frac{\pi}{4}$, the value of $\pi$ can be approximated using a Monte Carlo method:
\begin{enumerate}
    \item Draw a square, then inscribe a quadrant within it;
    \item Uniformly scatter a given number of points over the square;
    \item Count the number of points inside the quadrant, i.e. having a distance from the origin of less than 1. The ratio of the inside-count and the total-sample-count is an estimate of the ratio of the two areas, namely the approximation of $\frac{\pi}{4}$;
    \item Multiply the result by 4 to estimate $\pi$.
\end{enumerate}
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.7]{figure/estimating-pi-monte-carlo-method.png}
    % \caption{Caption}
    % \label{fig:my_label}
\end{figure}
\end{example}

\begin{definition}
\boxed{\textbf{Normal distribution}} $X\sim N(\mu,\sigma^2)$ is defined as 
\begin{equation*}
     f(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}, F(x)=\int_0^\infty\frac{1}{\sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}dx,
 \end{equation*}
 where $\mu$ is the mean value and $\sigma$ is the standard deviation.
\end{definition}

\begin{remark}
Some properties of normal distribution:
\begin{itemize}
    \item If $X\sim N(\mu,\sigma^2), a, b$ are real, then $aX+b\sim N(a\mu+b,(a\sigma)^2)$.
    \item If $X\sim N(\mu_X,\sigma_X^2)$ and $Y\sim N(\mu_Y,\sigma_Y^2)$ are two independent random variables, then $ X+Y \sim N(\mu_X + \mu_Y, \sigma^2_X + \sigma^2_Y), {\displaystyle X-Y\sim N(\mu _{X}-\mu _{Y},\sigma _{X}^{2}+\sigma _{Y}^{2})}$.
\end{itemize}
\end{remark}

\begin{theorem}
\textbf{Law of large numbers(LLN).} When $\{X_1,\cdots,X_n\}$ is an infinite sequence of independent identical distributed random variables with expectation $E(X_1),\cdots,E(X_n)=\mu$, we can have
\begin{equation*}
    \Bar{X}_n\rightarrow\mu, \text{when }n\rightarrow\infty,
\end{equation*}
where $\Bar{X}_n=\frac{1}{n}\sum^n_{i=1} X_i$.
\end{theorem}

\begin{remark}
We can approximate the expectation of an unknown distribution by performing a sufficient number of the trials.
\end{remark}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.15]{figure/1920px-Lawoflargenumbers.svg.png}
    \caption{Law of large numbers}
    % \label{fig:my_label}
\end{figure}

\begin{theorem}
\textbf{Central limit theorem(CLT).} In many situation, when independent random variables are added, their properly normalized sum tends toward a normal distribution even if the original variables themselves are not normal distributed.
\end{theorem}

\begin{remark}
The theorem is a key concept in probability theory because this theorem allows us to leverage the normal distribution to solve other unknown distributions.
\end{remark}

\begin{example}
Given a sequence of i.i.d. random variables $\{X_1,\cdots,X_n\}$, we randomly pick $m$(relatively large) samples from the population to calculate the mean value and repeat it $k$ times. And the $k$ mean values would be normal distributed.
\end{example}

\begin{definition}
\boxed{\textbf{Confidence interval}} is defined as 

\end{definition}

\begin{definition}
\boxed{\textbf{Covariance}} for two random variables $X$ and $Y$, each with sample size $d$ is defined by
\begin{align*}
    \operatorname{cov}(X,Y)&=E[(X-E[X])(Y-E(Y])]\\
    &=\sum^d_{i=1}\frac{(x_i-\Bar{x})(y_i-\Bar{y})}{d}
\end{align*}
\end{definition}

\begin{definition}
\boxed{\textbf{Covariance matrix}} for $n$ sets of random variables $\{X^1\},\cdots,\{X^n\}$, each with sample size $d$ is defined by
\begin{align*}
    \Sigma=
    \begin{pmatrix}
        \operatorname{cov}(X^1,X^1) & \cdots & \operatorname{cov}(X^1,X^n)\\
        \vdots &  \ddots &  \vdots \\
        \operatorname{cov}(X^n,X^1) & \cdots & \operatorname{cov}(X^n,X^n)
    \end{pmatrix}
\end{align*}
\end{definition}

\newpage
\section{Deep Learning}
\subsection{Common terms}
\begin{definition}
One \boxed{\textbf{batch}} of samples is a group of samples concatenated together to go through the network before updating the model weights.
\end{definition}

\begin{table}[H]
\centering
\begin{tabular}{cc}
\hline
{ \textbf{Types}}              & { \textbf{Batch size}}                    \\ \hline
Stochastic gradient descent & 1 \\
Mini-Batch gradient descent & $(1, \text{size of training dataset})$\\
Batch gradient descent      & size of training dataset \\ \hline
\end{tabular}
\end{table}

\begin{example}
In PyTorch's implementation, mini-batch concatenates the designated samples together and put it into the network. For instance, if the image is of size 128*128*128, 4 channels, and mini-batch size is 2, then the shape of input tensor during training is 2*4*128*128*128. 

You may wonder that when it comes to testing, the input is only one sample at a time, but the model stays the same, how can that still work? The things is, a model only regulates the shape of convolution kernel, pooling parameters, activation function and how they are organized, the size of the input does not matter, it can either 2*4*128*128*128 or 1*4*128*128*128. The final loss is actually summation of all samples' loss. As long as the output size matches the ground-truth size, the workflow can be performed uninterruptedly.
\end{example}

\begin{definition}
One \boxed{\textbf{iteration}} is a time span when an a batch of training data is passed forward and backward through the neural network.
\end{definition}

\begin{definition}
One \boxed{\textbf{epoch}} is a time span when an entire training dataset is passed forward and backward through the neural network.
\end{definition}

\begin{definition}
In \boxed{\textbf{convolutional layer}}, multiple channels of feature maps were extracted by sliding the trainable convolutional kernels across the input feature maps.
\end{definition}

\begin{definition}
\boxed{\textbf{Max pooling layer}} is a way to reduce the image sizes, to provide an abstracted form of the representation, to reduce the computational cost and to promote spatial invariance of the network.
\end{definition}

\begin{definition}
\boxed{\textbf{Fully-connected input layer}} takes the output of the previous layers, ``flattens'' them and turns them into a single vector that can be an input for the next stage.
\end{definition}

\begin{definition}
\boxed{\textbf{Dropout layer}} is used to alleviate data overfitting.
\end{definition}

\begin{table}[H]
\centering
\caption{VGGNet in detail}
\begin{tabular}{llll}
\hline
\multicolumn{1}{c}{\textbf{Layer}} & \multicolumn{1}{c}{\textbf{Size}} & \multicolumn{1}{c}{\textbf{Memory}} & \multicolumn{1}{c}{\textbf{Weights}} \\ \hline
INPUT                              & {[}224x224x3{]}                   & 224*224*3=150K                      & 0                        \\
CONV3-64                           & {[}224x224x64{]}                  & 224*224*64=3.2M                     & (3*3*3)*64               \\
CONV3-64                           & {[}224x224x64{]}                  & 224*224*64=3.2M                     & (3*3*64)*64              \\
POOL2                              & {[}112x112x64{]}                  & 112*112*64=800K                     & 0                        \\
CONV3-128                          & {[}112x112x128{]}                 & 112*112*128=1.6M                    & (3*3*64)*128             \\
CONV3-128                          & {[}112x112x128{]}                 & 112*112*128=1.6M                    & (3*3*128)*128            \\
POOL2                              & {[}56x56x128{]}                   & 56*56*128=400K                      & 0                        \\
CONV3-256                          & {[}56x56x256{]}                   & 56*56*256=800K                      & (3*3*128)*256            \\
CONV3-256                          & {[}56x56x256{]}                   & 56*56*256=800K                      & (3*3*256)*256            \\
CONV3-256                          & {[}56x56x256{]}                   & 56*56*256=800K                      & (3*3*256)*256            \\
POOL2                              & {[}28x28x256{]}                   & 28*28*256=200K                      & 0                        \\
CONV3-512                          & {[}28x28x512{]}                   & 28*28*512=400K                      & (3*3*256)*512            \\
CONV3-512                          & {[}28x28x512{]}                   & 28*28*512=400K                      & (3*3*256)*512            \\
CONV3-512                          & {[}28x28x512{]}                   & 28*28*512=400K                      & (3*3*256)*512            \\
POOL2                              & {[}14x14x512{]}                   & 14*14*512=100K                      & 0                        \\
CONV3-512                          & {[}14x14x512{]}                   & 14*14*512=100K                      & (3*3*512)*512            \\
CONV3-512                          & {[}14x14x512{]}                   & 14*14*512=100K                      & (3*3*512)*512            \\
CONV3-512                          & {[}14x14x512{]}                   & 14*14*512=100K                      & (3*3*512)*512            \\
POOL2                              & {[}7x7x512{]}                     & 7*7*512=25K                         & 0                        \\
FC                                 & {[}1x1x4096{]}                    & 4096                                & 7*7*512*4096             \\
FC                                 & {[}1x1x4096{]}                    & 4096                                & 4096*4096                \\
FC                                 & {[}1x1x1000{]}                    & 1000                                & 4096*1000                \\ \hline
\end{tabular}
\end{table}

\subsection{Loss Functions}
\begin{definition}
\boxed{\textbf{Intersection of Union(Jaccard index, IoU)}} is defined as 
\begin{equation*}
    \operatorname{IoU}=\frac{|A\cap B|}{|A\cup B|}
\end{equation*}
\end{definition}

\begin{definition}
\boxed{\textbf{Dice loss function}} is defined as 
\begin{equation*}
    L_{\operatorname{dice}}=-\frac{2|A\cap B|}{|A|+|B|}=-\frac{2\times\left<v_{\operatorname{true}}, v_{\operatorname{pred}}\right>}{\|v_{\operatorname{true}}\|^2_2+\|v_{\operatorname{pred}}\|^2_2+\epsilon}\in(-1,0],
\end{equation*}
where $v_{\operatorname{true}},v_{\operatorname{pred}}\in\mathbb{R}^{h\times w\times d}$ are \textbf{one-hot vectors}, and $\epsilon$ is a small constant to avoid zero division. 
\end{definition}

\begin{remark}
Since dice loss function is a loss function, just as other loss function, lower score indicates better alignment. And that's the reason why the negative sign is indispensable.
\end{remark}

\begin{definition}
\boxed{\textbf{$L^1$ loss function}} is defined as 
\begin{equation*}
    L_{L^1}=\|v_{\operatorname{true}}-v_{\operatorname{pred}}\|_1,
\end{equation*}
where $v_{\operatorname{true}},v_{\operatorname{pred}}$ are vectors.
\end{definition}

\begin{definition}
\boxed{\textbf{$L^2$ loss function}} is defined as 
\begin{equation*}
    L_{L^2}=\|v_{\operatorname{true}}-v_{\operatorname{pred}}\|^2_2,
\end{equation*}
where $v_{\operatorname{true}},v_{\operatorname{pred}}$ are vectors.
\end{definition}

\begin{definition}
\boxed{\textbf{Hausdorff distance}} is defined as 
\begin{equation*}
     d_{\mathrm {H} }(X,Y)=\max \left\{\,\sup _{x\in X}\inf _{y\in Y}d(x,y),\,\sup _{y\in Y}\inf _{x\in X}d(x,y)\,\right\}
\end{equation*}
where X and Y be two non-empty subsets of a metric space $(M,d)$.
\end{definition}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{figure/hausdorff.png}
    \caption{Components of the calculation of the Hausdorff distance between the green line X and the blue line Y.}
\end{figure}

\begin{definition}
\boxed{\textbf{Entropy}} of the \textbf{discrete probability distribution} $p$ is defined as
\begin{equation*}
   H(p)=-\sum _{x\in {\mathcal {X}}}{p (x)\log p (x)} \in[0,1]
\end{equation*}

\begin{remark}
If entropy is closer to 1, means the distribution is of high level of disorder (low level of purity). If entropy is closer to 0, means the distribution is of low level of disorder (high level of purity).
\end{remark}

\begin{definition}
\boxed{\textbf{Cross-entropy loss function}} of the \textbf{discrete probability distribution} $p$ and $q$ over a given set is defined as 
\begin{equation*}
    H(p,q)=-\sum_{x\in {\mathcal {X}}} p(x)\log(q(x))=-\left<p,\log(q)\right>\in[0,\infty),
\end{equation*}
which measures the distance between the distributions.
\end{definition}

\begin{remark}
Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a \textbf{probability value between 0 and 1}. Cross-entropy loss increases as the predicted probability diverges from the actual label. 
\end{remark}

\end{definition}

\begin{example}
As for the two discrete distributions below:
\begin{table}[H]
\centering
\begin{tabular}{cccc}
\hline
\textbf{Distribution} & \textbf{Class A} & \textbf{Class B} & \textbf{Class C} \\ \hline
$p$            & 0       & 0       & 1    \\ 
$q$           & 0.1     & 0.2     & 0.7        \\ \hline
\end{tabular}
\end{table}
we can have the cross entropy 
\begin{equation*}
    H(p,q)=-(0\times\log(0.1)+0\times\log(0.2)+0\times\log(0.7))=0.35
\end{equation*}
\end{example}

\begin{remark}
According to the example above, the only thing that contributes to the value of loss is the predicted possibility of the ground-truth class, since the possibilities of other classes are all wiped out by 0. The closer predicted possibility of the ground-truth class to 1, the lower loss will be.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.25]{figure/log.jpg}
    \caption{$y=-\log(x)$}
    \label{fig:my_label}
\end{figure}
\end{remark}

\begin{definition}
\boxed{\textbf{Kullbackâ€“Leibler(KL) divergence}} is defined as
\begin{align*}
    D_{\text{KL}}(p,q)&=\sum _{x\in {\mathcal {X}}}p(x)\log \left({\frac {p(x)}{q(x)}}\right)\\
    &=-\sum _{x\in {\mathcal {X}}}p(x)\log q(x)+\sum _{x\in {\mathcal {X}}}p(x)\log p(x)\\
    &=-H(p,q)+H(p)\in[0,\infty),
\end{align*}
\end{definition}

\begin{remark}
KL divergence describes how different $q$ is from $p$ from the perspective of $p$. When $p$ is fixed, just as the ground-truth data in training, minimizing KL divergence and cross-entropy is equivalent, as $H(p)$ is a constant.
\end{remark}

\begin{example}
As for the two discrete distributions below:
\begin{table}[H]
\centering
\begin{tabular}{cccc}
\hline
\textbf{Distribution} & \textbf{Class A} & \textbf{Class B} & \textbf{Class C} \\ \hline
$p$            & 0       & 0       & 1    \\ 
$q$           & 0.1     & 0.2     & 0.7        \\ \hline
\end{tabular}
\end{table}
we can have the KL divergence
\begin{equation*}
    D_{\operatorname{KL}}(p,q)=(0\times\log(0)+0\times\log(0)+1\times\log(1))-(0\times\log(0.1)+0\times\log(0.2)+0\times\log(0.7))=0.35
\end{equation*}
\end{example}

\subsection{Normalization}
Normalization is used to reduce internal covariate shift among the training samples.
\begin{definition}
\boxed{\textbf{Batch normalization}}\cite{batch} is defined as
\begin{align*}
     \mu_{c}&=\frac{1}{HWN}\sum^N_n\sum^W_w\sum^H_h x_{cnhw}\\
     \sigma_{c}^2&=\frac{1}{HWN}\sum^N_n\sum^W_w\sum^H_h(x_{cnhw}-\mu_c)^2\\
     \hat{x}_{c}&=\frac{x_{c}-\mu_{c}}{\sqrt{\sigma_{c}^2+\epsilon}}
\end{align*}
where $x$ are the values of input over a mini-batch, $C,N,H,W$ are the channel, batch, height, width size, respectively. Batch normalization should be performed by each channel.
\end{definition}

\begin{definition}
\boxed{\textbf{Layer normalization}} is defined as
\begin{align*}
     \mu_{n}&=\frac{1}{HWC}\sum^C_c\sum^W_w\sum^H_h x_{cnhw}\\
     \sigma_{n}^2&=\frac{1}{HWC}\sum^C_c\sum^W_w\sum^H_h(x_{cnhw}-\mu_n)^2\\
     \hat{x}_{n}&=\frac{x_{n}-\mu_{n}}{\sqrt{\sigma_{n}^2+\epsilon}}
\end{align*}
where $x$ are the values of input over a mini-batch, $C,N,H,W$ are the channel, batch, height, width size, respectively. Layer normalization should be performed by each sample in a mini-batch.
\end{definition}

\begin{definition}
\boxed{\textbf{Instance normalization}} is defined as
\begin{align*}
     \mu_{cn}&=\frac{1}{HW}\sum^W_w\sum^H_h x_{cnhw}\\
     \sigma_{cn}^2&=\frac{1}{HW}\sum^W_w\sum^H_h(x_{cnhw}-\mu_{cn})^2\\
     \hat{x}_{cn}&=\frac{x_{cn}-\mu_{cn}}{\sqrt{\sigma_{cn}^2+\epsilon}}
\end{align*}
where $x$ are the values of input over a mini-batch, $C,N,H,W$ are the channel, batch, height, width size, respectively. Instance normalization should be performed by each channel and sample in a batch.
\end{definition}

\begin{definition}
\boxed{\textbf{Group normalization}}\cite{group} is a middle ground between layer and instance normalization, which is defined as
\begin{align*}
     \mu_{gn}&=\frac{1}{HW|G_i|}\sum^{|G_i|}_{c\in G_i}\sum^W_w\sum^H_h x_{cnhw}\\
     \sigma_{gn}^2&=\frac{1}{HW|G_i|}\sum^{|G_i|}_{c\in G_i}\sum^W_w\sum^H_h(x_{cnhw}-\mu_n)^2\\
     \hat{x}_{gn}&=\frac{x_{n}-\mu_{n}}{\sqrt{\sigma_{n}^2+\epsilon}}\\
     G&=\{G_1,\cdots\}\\
     G_i&=\{c_i,\cdots,c_{i'}\}
\end{align*}
where $x$ are the values of input over a mini-batch, $C,N,H,W,|G|$ are the channel, batch, height, width, group size, respectively. Instance normalization should be performed by each group among the channel set.
\end{definition}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{figure/normalization.png}
    \caption{Normalization methods. Each subplot shows a feature map tensor, with $N$ as the batch axis, $C$ as the channel axis, and $(H, W)$ as the spatial axes. The pixels in blue are normalized by the same mean and variance, computed by aggregating the values of these pixels.}
    \label{fig:my_label}
\end{figure}

% \subsection{Optimizer}
% \begin{definition}
% \boxed{\textbf{Adam optimizer}}
% \end{definition}
% \subsection{Regularization}
\subsection{Activation Function}
\begin{table}[H]
\centering
\begin{tabular}{cccc}
\hline
\textbf{Types}                    & \textbf{Functions}                                       & \textbf{Range}                                           & \textbf{Order of continuity} \\ \hline
Sigmoid(logistic)        & $\sigma(x)=\frac {1}{1+e^{-x}}$               & $(0,1)$  & $C^\infty$\\
Tanh(hyperbolic tangent) & $\tanh(x)={\frac {e^{x}-e^{-x}}{e^{x}+e^{-x}}}$ & $(-1,1)$   & $C^\infty$\\
ReLU(rectified linear unit)  & $\operatorname{ReLU}(x)=\max\{0,x\}$            & $[0,+\infty)$    & $C^0$\\       
Leaky ReLU               & $\operatorname{Leaky ReLU}(x)=\max\{0.01x,x\}$  & $(-\infty,+\infty)$ & $C^0$\\
ELU(exponential linear unit)& ${\begin{cases}\alpha \left(e^{x}-1\right)&{\text{if }}x\leq 0\\x&{\text{if }}x>0\end{cases}}$ & $(-\alpha,+\infty)$ & $\begin{cases}C^{1}&{\text{if }}\alpha =1\\C^{0}&{\text{otherwise}}\end{cases}$\\ 
GELU(gaussian error linear unit)& $\begin{aligned}&{\frac {1}{2}}x\left(1+{\text{erf}}\left({\frac {x}{\sqrt {2}}}\right)\right)\end{aligned}$ & $(-0.17,+\infty)$ & $C^\infty$\\
\hline
\end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{figure/sigmoid.png}
    \includegraphics[scale=0.6]{figure/tanh.png}
    \includegraphics[scale=0.6]{figure/relu.png}
    \includegraphics[scale=0.6]{figure/leakyrelu.png}
    \includegraphics[scale=0.6]{figure/elu.png}
    \includegraphics[scale=0.08]{figure/gelu.png}
    \caption{Plots of sigmoid, tahn, ReLU, Leaky ReLU, ELU, GELU activation functions, correspondingly.}
\end{figure}

The following table lists activation functions that are not functions of a single fold $x$ from the previous layer or layers:
\begin{table}[H]
\centering
\begin{tabular}{cccc}
\hline
{ \textbf{Types}} & { \textbf{Functions}}                            & { \textbf{Range}}        & { \textbf{Order of continuity}} \\ \hline
{ Softmax}        & { $\frac {e^{x_{i}}}{\sum _{j=1}^{J}e^{x_{j}}}$} & { $(0,1)$}               & { $C^{\infty }$}                \\ 
{ Maxout}         & { $\max _{i}x_{i}$}                              & { $ (-\infty ,\infty )$} & { $C^{0}$}                      \\ \hline
\end{tabular}
\end{table}

\begin{remark}
Activation functions have different mathematical properties:
\begin{itemize}
    \item \textbf{Nonlinear.} When the activation function is non-linear, then a two-layer neural network can be proven to be a universal function approximator. This is known as the Universal Approximation Theorem. The identity activation function does not satisfy this property. When multiple layers use the identity activation function, the entire network is equivalent to a single-layer model.
    \item \textbf{Range.} When the range of the activation function is finite, gradient-based training methods tend to be more stable, because pattern presentations significantly affect only limited weights. When the range is infinite, training is generally more efficient because pattern presentations significantly affect most of the weights. In the latter case, smaller learning rates are typically necessary.
    \item \textbf{Continuously differentiable.} This property is desirable (ReLU is not continuously differentiable and has some issues with gradient-based optimization, but it is still possible) for enabling gradient-based optimization methods. The binary step activation function is not differentiable at 0, and it differentiates to 0 for all other values, so gradient-based methods can make no progress with it.
    \item \textbf{Monotonic.} When the activation function is monotonic, the error surface associated with a single-layer model is guaranteed to be convex.
\end{itemize}
\end{remark}
% \subsection{Data Augmentation}


\newpage
\begin{thebibliography}{99} 
\bibitem{batch}Ioffe, S. and Szegedy, C., 2015, June. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning (pp. 448-456). PMLR.
\bibitem{group}Wu, Y. and He, K., 2018. Group normalization. In Proceedings of the European conference on computer vision (ECCV) (pp. 3-19).
\end{thebibliography}
\end{document}